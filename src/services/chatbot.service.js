import dotenv from "dotenv";
dotenv.config();

import OpenAI from "openai";
import Tenant from "../models/Tenant.js";
import Instruction from "../models/Instruction.js"; // ğŸ‘ˆ necesario

if (!process.env.OPENAI_API_KEY) {
  console.error("âŒ No se encontrÃ³ OPENAI_API_KEY. Verifica tu archivo .env");
}

const client = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

/**
 * Genera la respuesta del chatbot usando OpenAI.
 * Incluye el prompt del tenant + instrucciones especÃ­ficas desde MongoDB.
 */
export async function generateChatbotReply(
  userMessage,
  instructions = [],
  tenant = null,
  languageFromBody = null
) {
  try {
    // ğŸ”’ Siempre aseguramos que 'instructions' sea un array
    const safeInstructions = Array.isArray(instructions) ? instructions : [];

    // ğŸ§  Determinar idioma
    const language = languageFromBody || tenant?.language || "es";

    // ğŸ“‹ Prompt base del tenant
    const promptBase =
      tenant?.prompt ||
      `Eres el asistente oficial de ${tenant?.name || "un parque temÃ¡tico"}.
       Ofreces informaciÃ³n sobre horarios, precios, atracciones y servicios.
       SÃ© breve (mÃ¡x. 3 frases).`;

    // ğŸ§© Combinar instrucciones del tenant (si existen)
    const combinedInstructions = safeInstructions.length
      ? `\nSigue estas instrucciones adicionales:\n- ${safeInstructions.join("\n- ")}`
      : "";

    // ğŸ’¬ Construir el prompt final
    const fullSystemPrompt = `${promptBase}${combinedInstructions}
    Tu apodo es ${tenant?.name || "NeuronicBot"}.
    Responde SIEMPRE en ${language}.`;

    // ğŸª¶ Log de depuraciÃ³n (verÃ¡s esto en tu terminal)
    console.log("ğŸ§© SYSTEM PROMPT ENVIADO A OPENAI:\n", fullSystemPrompt, "\n");

    // ğŸ—£ï¸ Mensajes enviados a OpenAI
    const messages = [
      { role: "system", content: fullSystemPrompt },
      { role: "user", content: userMessage },
    ];

    // ğŸš€ Llamada a OpenAI
    const completion = await client.chat.completions.create({
      model: "gpt-4o-mini",
      messages,
      temperature: tenant?.temperature || 0.7,
      max_tokens: 200,
    });

    // âœ… Respuesta limpia
    return completion.choices[0].message.content.trim();
  } catch (err) {
    console.error("âŒ Error generando respuesta:", err);
    return "Hubo un error al generar la respuesta del asistente.";
  }
}
